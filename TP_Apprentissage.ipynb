{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71b2df1a",
   "metadata": {},
   "source": [
    "# TP Apprentissage supervisé - Partie 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62f43f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "987cfe6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_raw = pd.read_csv('acsincome_ca_features.csv')\n",
    "label_raw = pd.read_csv('acsincome_ca_labels.csv', usecols=['PINCP'])[['PINCP']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5573f2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AGEP</th>\n",
       "      <th>COW</th>\n",
       "      <th>SCHL</th>\n",
       "      <th>MAR</th>\n",
       "      <th>OCCP</th>\n",
       "      <th>POBP</th>\n",
       "      <th>RELP</th>\n",
       "      <th>WKHP</th>\n",
       "      <th>SEX</th>\n",
       "      <th>RAC1P</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9610.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1970.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>65.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2040.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9610.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1021.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195660</th>\n",
       "      <td>38.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1021.0</td>\n",
       "      <td>210.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195661</th>\n",
       "      <td>39.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1021.0</td>\n",
       "      <td>210.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195662</th>\n",
       "      <td>61.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5240.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195663</th>\n",
       "      <td>69.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2040.0</td>\n",
       "      <td>207.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195664</th>\n",
       "      <td>40.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9600.0</td>\n",
       "      <td>303.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>195665 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        AGEP  COW  SCHL  MAR    OCCP   POBP  RELP  WKHP  SEX  RAC1P\n",
       "0       30.0  6.0  14.0  1.0  9610.0    6.0  16.0  40.0  1.0    8.0\n",
       "1       21.0  4.0  16.0  5.0  1970.0    6.0  17.0  20.0  1.0    1.0\n",
       "2       65.0  2.0  22.0  5.0  2040.0    6.0  17.0   8.0  1.0    1.0\n",
       "3       33.0  1.0  14.0  3.0  9610.0   36.0  16.0  40.0  1.0    1.0\n",
       "4       18.0  2.0  19.0  5.0  1021.0    6.0  17.0  18.0  2.0    1.0\n",
       "...      ...  ...   ...  ...     ...    ...   ...   ...  ...    ...\n",
       "195660  38.0  1.0  22.0  1.0  1021.0  210.0   0.0  40.0  1.0    6.0\n",
       "195661  39.0  1.0  22.0  1.0  1021.0  210.0   1.0  40.0  2.0    6.0\n",
       "195662  61.0  1.0  19.0  1.0  5240.0   17.0   0.0  45.0  1.0    1.0\n",
       "195663  69.0  7.0  24.0  1.0  2040.0  207.0   0.0  45.0  1.0    6.0\n",
       "195664  40.0  1.0  17.0  1.0  9600.0  303.0   0.0  40.0  1.0    8.0\n",
       "\n",
       "[195665 rows x 10 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d972100",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PINCP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195660</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195661</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195662</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195663</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195664</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>195665 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        PINCP\n",
       "0       False\n",
       "1       False\n",
       "2       False\n",
       "3       False\n",
       "4       False\n",
       "...       ...\n",
       "195660   True\n",
       "195661   True\n",
       "195662   True\n",
       "195663  False\n",
       "195664  False\n",
       "\n",
       "[195665 rows x 1 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04158843",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43ecae4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(X_init, y_init, ratio=0.04):\n",
    "    # We normalize the ages\n",
    "    scaler = RobustScaler()\n",
    "    age_scaled = scaler.fit_transform(X_init[[\"AGEP\"]])\n",
    "    X_init[[\"AGEP\"]] = age_scaled\n",
    "    \n",
    "    # There is no NA value. good.\n",
    "    \n",
    "    X_all, y_all = shuffle(X_init, y_init)\n",
    "    \n",
    "    # only use the first N samples to limit training time\n",
    "    num_samples = int(len(X_all)*ratio)\n",
    "    X, y = X_all.head(num_samples), y_all.head(num_samples)\n",
    "    \n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a4e3e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = preprocess(feat_raw, label_raw)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "y_train = y_train.to_numpy().reshape(1,-1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a7bf090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AGEP</th>\n",
       "      <th>COW</th>\n",
       "      <th>SCHL</th>\n",
       "      <th>MAR</th>\n",
       "      <th>OCCP</th>\n",
       "      <th>POBP</th>\n",
       "      <th>RELP</th>\n",
       "      <th>WKHP</th>\n",
       "      <th>SEX</th>\n",
       "      <th>RAC1P</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36859</th>\n",
       "      <td>-0.60</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7200.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157679</th>\n",
       "      <td>-0.32</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9640.0</td>\n",
       "      <td>303.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174422</th>\n",
       "      <td>0.36</td>\n",
       "      <td>7.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3090.0</td>\n",
       "      <td>213.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31389</th>\n",
       "      <td>-0.88</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4700.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67833</th>\n",
       "      <td>-0.04</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6700.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112337</th>\n",
       "      <td>0.52</td>\n",
       "      <td>3.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4220.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151611</th>\n",
       "      <td>0.28</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4020.0</td>\n",
       "      <td>303.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158865</th>\n",
       "      <td>0.28</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4810.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14642</th>\n",
       "      <td>-0.08</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9620.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123170</th>\n",
       "      <td>0.20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9645.0</td>\n",
       "      <td>313.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6260 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        AGEP  COW  SCHL  MAR    OCCP   POBP  RELP  WKHP  SEX  RAC1P\n",
       "36859  -0.60  1.0  19.0  5.0  7200.0    6.0   4.0  40.0  1.0    8.0\n",
       "157679 -0.32  1.0  16.0  1.0  9640.0  303.0   1.0  30.0  2.0    8.0\n",
       "174422  0.36  7.0  23.0  5.0  3090.0  213.0   0.0  70.0  1.0    1.0\n",
       "31389  -0.88  1.0  19.0  5.0  4700.0    6.0   2.0  40.0  2.0    9.0\n",
       "67833  -0.04  1.0  18.0  1.0  6700.0    6.0   1.0  40.0  1.0    1.0\n",
       "...      ...  ...   ...  ...     ...    ...   ...   ...  ...    ...\n",
       "112337  0.52  3.0  16.0  1.0  4220.0    6.0   1.0  40.0  1.0    1.0\n",
       "151611  0.28  1.0   8.0  1.0  4020.0  303.0   1.0  40.0  2.0    1.0\n",
       "158865  0.28  1.0  19.0  1.0  4810.0    6.0   1.0  40.0  1.0    1.0\n",
       "14642  -0.08  1.0  16.0  3.0  9620.0    6.0   4.0  30.0  1.0    1.0\n",
       "123170  0.20  1.0  17.0  1.0  9645.0  313.0   0.0  40.0  1.0    8.0\n",
       "\n",
       "[6260 rows x 10 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7cb3ac",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9c85d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.69728435 0.71086262 0.71884984 0.70686901 0.7100639 ]\n"
     ]
    }
   ],
   "source": [
    "# SVM, RandomForest, AdaBoost et GradientBoosting\n",
    "model = SVC()\n",
    "cv_results = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "print(cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e982c75b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.71      0.84      0.77       904\n",
      "        True       0.71      0.54      0.61       662\n",
      "\n",
      "    accuracy                           0.71      1566\n",
      "   macro avg       0.71      0.69      0.69      1566\n",
      "weighted avg       0.71      0.71      0.71      1566\n",
      "\n",
      "-----------------------------------\n",
      "[[761 143]\n",
      " [306 356]]\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"-----------------------------------\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "# [[TP, FN],\n",
    "#  [FP, TN]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd73d60e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.0107977516232771, 'degree': 2, 'kernel': 'rbf'}\n",
      "0.7124600638977635\n"
     ]
    }
   ],
   "source": [
    "params = {'kernel': ['poly', 'rbf', 'sigmoid'],\n",
    "         'degree': [2,3],\n",
    "         'C': np.logspace(-2.3, -1.3, 10)}\n",
    "\n",
    "gsv = GridSearchCV(SVC(), param_grid=params, cv=5)\n",
    "gsv.fit(X_train, y_train)\n",
    "\n",
    "print(gsv.best_params_)\n",
    "print(gsv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d8f244f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.71      0.85      0.77       904\n",
      "        True       0.72      0.52      0.61       662\n",
      "\n",
      "    accuracy                           0.71      1566\n",
      "   macro avg       0.72      0.69      0.69      1566\n",
      "weighted avg       0.71      0.71      0.70      1566\n",
      "\n",
      "-----------------------------------\n",
      "[[772 132]\n",
      " [317 345]]\n"
     ]
    }
   ],
   "source": [
    "model = gsv.best_estimator_\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"-----------------------------------\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "# [[TP, FN],\n",
    "#  [FP, TN]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb277d4",
   "metadata": {},
   "source": [
    "# RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "50a34532",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = preprocess(feat_raw, label_raw, 0.01)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "y_train = y_train.to_numpy().reshape(1,-1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dacda698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.77316294 0.79552716 0.81789137 0.7827476  0.76602564]\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier()\n",
    "cv_results = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "print(cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b4f49fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.84      0.86      0.85       231\n",
      "        True       0.79      0.76      0.78       161\n",
      "\n",
      "    accuracy                           0.82       392\n",
      "   macro avg       0.81      0.81      0.81       392\n",
      "weighted avg       0.82      0.82      0.82       392\n",
      "\n",
      "-----------------------------------\n",
      "[[198  33]\n",
      " [ 38 123]]\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"-----------------------------------\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "# [[TP, FN],\n",
    "#  [FP, TN]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7d9cb98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 130, 'n_jobs': -1}\n",
      "0.8043335790939624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/insa/anaconda/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
      "450 fits failed out of a total of 2250.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "450 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/insa/anaconda/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/usr/local/insa/anaconda/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\", line 340, in fit\n",
      "    self._validate_params()\n",
      "  File \"/usr/local/insa/anaconda/lib/python3.11/site-packages/sklearn/base.py\", line 600, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/usr/local/insa/anaconda/lib/python3.11/site-packages/sklearn/utils/_param_validation.py\", line 97, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_depth' parameter of RandomForestClassifier must be an int in the range [1, inf) or None. Got 'None' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/usr/local/insa/anaconda/lib/python3.11/site-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.76150979 0.76724625 0.76725444 0.77300729 0.76086467 0.77620832\n",
      " 0.77045343 0.76597649 0.7781273  0.76725649 0.77364422 0.76534161\n",
      " 0.77236831 0.77172729 0.76533956 0.77300729 0.77237446 0.76917343\n",
      " 0.77172934 0.7698165  0.76789547 0.77237036 0.76534366 0.76916933\n",
      " 0.77109036 0.76917752 0.77876014 0.77492627 0.77045138 0.77364627\n",
      " 0.7883448  0.78451503 0.78323503 0.78323298 0.78067093 0.78386786\n",
      " 0.77939297 0.78194479 0.78579299 0.78451298 0.77747604 0.7825981\n",
      " 0.78514787 0.78130786 0.78962071 0.77811092 0.77747604 0.78387401\n",
      " 0.7813099  0.78131195 0.7845007  0.78003195 0.77491808 0.78322684\n",
      " 0.78322889 0.7870607  0.77556115 0.78707094 0.78386581 0.77556115\n",
      " 0.78452118 0.79600639 0.78962276 0.78835095 0.79409765 0.79793356\n",
      " 0.79985664 0.79473663 0.80048538 0.79985254 0.79986483 0.79218072\n",
      " 0.79282379 0.79921766 0.79026583 0.79537151 0.78898378 0.79920947\n",
      " 0.7979479  0.79154583 0.79730073 0.7966638  0.79665561 0.79601868\n",
      " 0.79602073 0.80049357 0.79793766 0.79218481 0.79346277 0.79921357\n",
      " 0.79346072 0.79027402 0.79346072 0.79985869 0.79730483 0.79154788\n",
      " 0.79921971 0.79410584 0.79729868 0.80114279 0.79219096 0.79090071\n",
      " 0.78643606 0.80050586 0.8011305  0.79409765 0.79666585 0.7979479\n",
      " 0.79793766 0.79730687 0.7896289  0.79346891 0.79602277 0.79154174\n",
      " 0.79217457 0.79536946 0.79538175 0.79473867 0.79921152 0.80049767\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.76917547 0.76533956 0.76597649 0.76534161 0.77109241 0.77493242\n",
      " 0.76726059 0.76725444 0.76981855 0.76981035 0.7627775  0.76405956\n",
      " 0.77364832 0.76917138 0.76533956 0.77045548 0.7685365  0.77237036\n",
      " 0.76981445 0.76598058 0.7640616  0.76214467 0.77364627 0.76214672\n",
      " 0.77365446 0.76534161 0.7755673  0.76533956 0.77237036 0.77364627\n",
      " 0.77748013 0.77939297 0.78067707 0.78003604 0.7755632  0.77556115\n",
      " 0.78067912 0.77683911 0.78132014 0.78323093 0.78259196 0.78194683\n",
      " 0.78451298 0.78195503 0.78132014 0.78003809 0.77684525 0.78132219\n",
      " 0.78004014 0.78004219 0.77876423 0.77876423 0.78195298 0.78067093\n",
      " 0.77939502 0.78322889 0.78131195 0.78003809 0.77492627 0.78323093\n",
      " 0.7966679  0.79090276 0.79985664 0.79729663 0.79026378 0.79475301\n",
      " 0.79665561 0.79153764 0.79793356 0.79729254 0.79730073 0.79729049\n",
      " 0.79729254 0.79601253 0.79472843 0.79218072 0.80305562 0.80048947\n",
      " 0.79154174 0.79154379 0.79857254 0.79090481 0.79538994 0.79346072\n",
      " 0.79026378 0.79089662 0.79282379 0.79729049 0.79217867 0.79793561\n",
      " 0.79346277 0.79474072 0.79601868 0.79090686 0.7953838  0.79601868\n",
      " 0.79474277 0.80369051 0.79153764 0.79474072 0.79730278 0.7966597\n",
      " 0.79666585 0.79858278 0.79602073 0.7966679  0.79729459 0.79218072\n",
      " 0.7953838  0.79857049 0.79602277 0.79281969 0.7998505  0.80304948\n",
      " 0.80049562 0.79346482 0.79281969 0.79857459 0.78834685 0.79793151\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.77045548 0.76150365 0.77365036 0.77493037 0.76788523 0.77109036\n",
      " 0.77045752 0.77364832 0.77876219 0.77300524 0.76470263 0.7685324\n",
      " 0.76342672 0.76597649 0.76725854 0.77173548 0.76789957 0.76470058\n",
      " 0.77173138 0.77109036 0.76661342 0.77300934 0.77109445 0.77236627\n",
      " 0.76853035 0.7685365  0.77300729 0.76917343 0.7685324  0.77620628\n",
      " 0.78387196 0.78003809 0.78898788 0.77684525 0.78770173 0.78068117\n",
      " 0.78067502 0.7825981  0.78131605 0.78259196 0.78195093 0.7755632\n",
      " 0.77748628 0.78579299 0.78514582 0.77492627 0.77875195 0.782594\n",
      " 0.78067093 0.78515196 0.77939297 0.78386991 0.77876014 0.78003809\n",
      " 0.77939297 0.78130786 0.77875399 0.77748013 0.77939297 0.781314\n",
      " 0.79793766 0.79473663 0.79602277 0.79345867 0.79857868 0.80112845\n",
      " 0.80432129 0.79665356 0.79792947 0.80177562 0.79346072 0.79665356\n",
      " 0.79346482 0.79921357 0.78642992 0.7953756  0.7998464  0.7940956\n",
      " 0.79601049 0.79218276 0.7966638  0.79666175 0.79282379 0.79025969\n",
      " 0.79473048 0.79410379 0.79537765 0.79858073 0.79346277 0.79730278\n",
      " 0.79986074 0.80113255 0.7953838  0.79921561 0.79985869 0.79411198\n",
      " 0.79921766 0.79985459 0.80433358 0.79409765 0.79857254 0.80050176\n",
      " 0.79729459 0.79472639 0.79985254 0.79474482 0.80112231 0.79218481\n",
      " 0.79665561 0.79537765 0.78961661 0.7998505  0.80048947 0.80113664\n",
      " 0.79282174 0.79154583 0.80368641 0.7953797  0.7953756  0.79218481]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "params = {'n_estimators': np.arange(50,150,10),\n",
    "         'criterion': ['gini', 'log_loss', 'entropy'],\n",
    "          'max_depth': ['None', 2,4,8,10],\n",
    "         'min_samples_split': [2,3,4],\n",
    "         'n_jobs':[-1]}\n",
    "\n",
    "gsv = GridSearchCV(RandomForestClassifier(), param_grid=params, cv=5)\n",
    "gsv.fit(X_train, y_train)\n",
    "\n",
    "print(gsv.best_params_)\n",
    "print(gsv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7b8dca3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.85      0.86      0.85       231\n",
      "        True       0.79      0.78      0.78       161\n",
      "\n",
      "    accuracy                           0.82       392\n",
      "   macro avg       0.82      0.82      0.82       392\n",
      "weighted avg       0.82      0.82      0.82       392\n",
      "\n",
      "-----------------------------------\n",
      "[[198  33]\n",
      " [ 36 125]]\n"
     ]
    }
   ],
   "source": [
    "model = gsv.best_estimator_\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"-----------------------------------\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "# [[TP, FN],\n",
    "#  [FP, TN]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2541c22",
   "metadata": {},
   "source": [
    "# Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "64a9521e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = preprocess(feat_raw, label_raw, 0.01)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "y_train = y_train.to_numpy().reshape(1,-1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5f3e2e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.76038339 0.81789137 0.78913738 0.80830671 0.75641026]\n"
     ]
    }
   ],
   "source": [
    "model = AdaBoostClassifier()\n",
    "cv_results = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "print(cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1d330f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.82      0.78      0.80       237\n",
      "        True       0.69      0.74      0.71       155\n",
      "\n",
      "    accuracy                           0.77       392\n",
      "   macro avg       0.76      0.76      0.76       392\n",
      "weighted avg       0.77      0.77      0.77       392\n",
      "\n",
      "-----------------------------------\n",
      "[[185  52]\n",
      " [ 40 115]]\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"-----------------------------------\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "# [[TP, FN],\n",
    "#  [FP, TN]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fb936044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algorithm': 'SAMME.R', 'learning_rate': 0.05011872336272722, 'n_estimators': 140}\n",
      "0.789641189481445\n"
     ]
    }
   ],
   "source": [
    "params = {'n_estimators': np.arange(50,150,10),\n",
    "         'learning_rate': np.logspace(-2.3, -1.3, 10),\n",
    "          'algorithm': ['SAMME', 'SAMME.R']\n",
    "         }\n",
    "\n",
    "gsv = GridSearchCV(AdaBoostClassifier(), param_grid=params, cv=5)\n",
    "gsv.fit(X_train, y_train)\n",
    "\n",
    "print(gsv.best_params_)\n",
    "print(gsv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "52ee26f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.80      0.84      0.82       237\n",
      "        True       0.73      0.68      0.70       155\n",
      "\n",
      "    accuracy                           0.78       392\n",
      "   macro avg       0.77      0.76      0.76       392\n",
      "weighted avg       0.77      0.78      0.77       392\n",
      "\n",
      "-----------------------------------\n",
      "[[199  38]\n",
      " [ 50 105]]\n"
     ]
    }
   ],
   "source": [
    "model = gsv.best_estimator_\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"-----------------------------------\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "# [[TP, FN],\n",
    "#  [FP, TN]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f77895",
   "metadata": {},
   "source": [
    "# GradientBoosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "28c2f579",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = preprocess(feat_raw, label_raw, 0.01)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "y_train = y_train.to_numpy().reshape(1,-1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bc8fe11a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.79233227 0.80511182 0.79872204 0.76996805 0.80128205]\n"
     ]
    }
   ],
   "source": [
    "model = GradientBoostingClassifier()\n",
    "cv_results = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "print(cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e488d2f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.84      0.84      0.84       239\n",
      "        True       0.75      0.75      0.75       153\n",
      "\n",
      "    accuracy                           0.80       392\n",
      "   macro avg       0.79      0.79      0.79       392\n",
      "weighted avg       0.80      0.80      0.80       392\n",
      "\n",
      "-----------------------------------\n",
      "[[200  39]\n",
      " [ 39 114]]\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"-----------------------------------\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "# [[TP, FN],\n",
    "#  [FP, TN]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8220356a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'friedman_mse', 'learning_rate': 0.03880510732210184, 'loss': 'exponential', 'n_estimators': 130}\n",
      "0.7954063242401901\n"
     ]
    }
   ],
   "source": [
    "params = {'loss': ['log_loss', 'exponential'],\n",
    "         'learning_rate': np.logspace(-2.3, -1.3, 10),\n",
    "          'n_estimators': np.arange(50,150,10),\n",
    "         'criterion':['friedman_mse', 'squared_error']}\n",
    "\n",
    "gsv = GridSearchCV(GradientBoostingClassifier(), param_grid=params, cv=5)\n",
    "gsv.fit(X_train, y_train)\n",
    "\n",
    "print(gsv.best_params_)\n",
    "print(gsv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "912ee811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.86      0.84      0.85       239\n",
      "        True       0.76      0.78      0.77       153\n",
      "\n",
      "    accuracy                           0.82       392\n",
      "   macro avg       0.81      0.81      0.81       392\n",
      "weighted avg       0.82      0.82      0.82       392\n",
      "\n",
      "-----------------------------------\n",
      "[[201  38]\n",
      " [ 34 119]]\n"
     ]
    }
   ],
   "source": [
    "model = gsv.best_estimator_\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"-----------------------------------\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "# [[TP, FN],\n",
    "#  [FP, TN]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143fdd6c",
   "metadata": {},
   "source": [
    "# Collecting the best models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ef30ddde",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = preprocess(feat_raw, label_raw, 0.04)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "y_train = y_train.to_numpy().reshape(1,-1)[0]\n",
    "\n",
    "best_GB = GradientBoostingClassifier(criterion='friedman_mse', learning_rate=0.03880510732210184,\n",
    "                                     loss='exponential', n_estimators=130)\n",
    "\n",
    "best_AB = AdaBoostClassifier(algorithm='SAMME.R', learning_rate=0.05011872336272722, n_estimators=140)\n",
    "\n",
    "best_RF = RandomForestClassifier(criterion='entropy', max_depth=10, min_samples_split=2, n_estimators=130, n_jobs=-1)\n",
    "\n",
    "best_SVM = SVC(C=0.0107977516232771, degree=2, kernel='rbf')\n",
    "        \n",
    "best_models = [best_GB, best_AB, best_RF, best_SVM]\n",
    "\n",
    "for model in best_models:\n",
    "    model.fit(X_train, y_train)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f854658e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================\n",
      "GradientBoostingClassifier(learning_rate=0.03880510732210184,\n",
      "                           loss='exponential', n_estimators=130)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.70      0.77       294\n",
      "           1       0.54      0.76      0.63       137\n",
      "\n",
      "    accuracy                           0.72       431\n",
      "   macro avg       0.70      0.73      0.70       431\n",
      "weighted avg       0.76      0.72      0.73       431\n",
      "\n",
      "-----------------------------------\n",
      "[[205  89]\n",
      " [ 33 104]]\n",
      "===================================\n",
      "AdaBoostClassifier(learning_rate=0.05011872336272722, n_estimators=140)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.75      0.79       294\n",
      "           1       0.56      0.68      0.61       137\n",
      "\n",
      "    accuracy                           0.73       431\n",
      "   macro avg       0.70      0.71      0.70       431\n",
      "weighted avg       0.75      0.73      0.73       431\n",
      "\n",
      "-----------------------------------\n",
      "[[220  74]\n",
      " [ 44  93]]\n",
      "===================================\n",
      "RandomForestClassifier(criterion='entropy', max_depth=10, n_estimators=130,\n",
      "                       n_jobs=-1)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.66      0.75       294\n",
      "           1       0.52      0.79      0.63       137\n",
      "\n",
      "    accuracy                           0.70       431\n",
      "   macro avg       0.70      0.73      0.69       431\n",
      "weighted avg       0.76      0.70      0.71       431\n",
      "\n",
      "-----------------------------------\n",
      "[[195  99]\n",
      " [ 29 108]]\n",
      "===================================\n",
      "SVC(C=0.0107977516232771, degree=2)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.88      0.81       294\n",
      "           1       0.58      0.36      0.45       137\n",
      "\n",
      "    accuracy                           0.71       431\n",
      "   macro avg       0.66      0.62      0.63       431\n",
      "weighted avg       0.69      0.71      0.69       431\n",
      "\n",
      "-----------------------------------\n",
      "[[258  36]\n",
      " [ 87  50]]\n"
     ]
    }
   ],
   "source": [
    "# Go search the rest of the data\n",
    "feat_ne = pd.read_csv('acsincome_ne_allfeaturesTP2.csv')\n",
    "label_ne = pd.read_csv('acsincome_ne_labelTP2.csv', usecols=['PINCP'])[['PINCP']]\n",
    "\n",
    "\n",
    "X_ne, y_ne = preprocess(feat_ne, label_ne)\n",
    "\n",
    "for model in best_models:\n",
    "    print(\"===================================\")\n",
    "    print(model)\n",
    "    model.fit(X_train, y_train)   \n",
    "\n",
    "    y_pred_ne = model.predict(X_ne)\n",
    "    print(classification_report(y_ne, y_pred_ne))\n",
    "    print(\"-----------------------------------\")\n",
    "    print(confusion_matrix(y_ne, y_pred_ne))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "aeaf6476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================\n",
      "GradientBoostingClassifier(learning_rate=0.03880510732210184,\n",
      "                           loss='exponential', n_estimators=130)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.77      0.81       719\n",
      "           1       0.72      0.82      0.77       533\n",
      "\n",
      "    accuracy                           0.79      1252\n",
      "   macro avg       0.79      0.79      0.79      1252\n",
      "weighted avg       0.80      0.79      0.79      1252\n",
      "\n",
      "-----------------------------------\n",
      "[[551 168]\n",
      " [ 95 438]]\n",
      "===================================\n",
      "AdaBoostClassifier(learning_rate=0.05011872336272722, n_estimators=140)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.79      0.80       719\n",
      "           1       0.73      0.76      0.74       533\n",
      "\n",
      "    accuracy                           0.78      1252\n",
      "   macro avg       0.77      0.77      0.77      1252\n",
      "weighted avg       0.78      0.78      0.78      1252\n",
      "\n",
      "-----------------------------------\n",
      "[[567 152]\n",
      " [129 404]]\n",
      "===================================\n",
      "RandomForestClassifier(criterion='entropy', max_depth=10, n_estimators=130,\n",
      "                       n_jobs=-1)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.74      0.80       719\n",
      "           1       0.71      0.83      0.76       533\n",
      "\n",
      "    accuracy                           0.78      1252\n",
      "   macro avg       0.78      0.79      0.78      1252\n",
      "weighted avg       0.79      0.78      0.78      1252\n",
      "\n",
      "-----------------------------------\n",
      "[[535 184]\n",
      " [ 91 442]]\n",
      "===================================\n",
      "SVC(C=0.0107977516232771, degree=2)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.86      0.77       719\n",
      "           1       0.72      0.48      0.58       533\n",
      "\n",
      "    accuracy                           0.70      1252\n",
      "   macro avg       0.71      0.67      0.67      1252\n",
      "weighted avg       0.71      0.70      0.69      1252\n",
      "\n",
      "-----------------------------------\n",
      "[[621  98]\n",
      " [277 256]]\n"
     ]
    }
   ],
   "source": [
    "# Go search the rest of the data\n",
    "feat_co = pd.read_csv('acsincome_co_allfeaturesTP2.csv')\n",
    "label_co = pd.read_csv('acsincome_co_labelTP2.csv', usecols=['PINCP'])[['PINCP']]\n",
    "\n",
    "X_co, y_co = preprocess(feat_co, label_co)\n",
    "\n",
    "for model in best_models:\n",
    "    print(\"===================================\")\n",
    "    print(model)\n",
    "    model.fit(X_train, y_train)   \n",
    "\n",
    "    y_pred_co = model.predict(X_co)\n",
    "    print(classification_report(y_co, y_pred_co))\n",
    "    print(\"-----------------------------------\")\n",
    "    print(confusion_matrix(y_co, y_pred_co))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8e1bdfdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AGEP</th>\n",
       "      <td>0.253918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>COW</th>\n",
       "      <td>0.051095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SCHL</th>\n",
       "      <td>0.357686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAR</th>\n",
       "      <td>-0.249854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OCCP</th>\n",
       "      <td>-0.342126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>POBP</th>\n",
       "      <td>-0.096035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RELP</th>\n",
       "      <td>-0.225071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WKHP</th>\n",
       "      <td>0.338617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SEX</th>\n",
       "      <td>-0.123547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAC1P</th>\n",
       "      <td>-0.095033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          target\n",
       "AGEP    0.253918\n",
       "COW     0.051095\n",
       "SCHL    0.357686\n",
       "MAR    -0.249854\n",
       "OCCP   -0.342126\n",
       "POBP   -0.096035\n",
       "RELP   -0.225071\n",
       "WKHP    0.338617\n",
       "SEX    -0.123547\n",
       "RAC1P  -0.095033\n",
       "target  1.000000"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = preprocess(feat_raw, label_raw, 0.04)\n",
    "X['target'] = y\n",
    "\n",
    "X.corr()[['target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1cb49287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_gb</th>\n",
       "      <th>pred_rf</th>\n",
       "      <th>pred_svm</th>\n",
       "      <th>pred_ab</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AGEP</th>\n",
       "      <td>0.248065</td>\n",
       "      <td>0.245363</td>\n",
       "      <td>0.089803</td>\n",
       "      <td>0.181025</td>\n",
       "      <td>0.296774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>COW</th>\n",
       "      <td>0.098516</td>\n",
       "      <td>0.073678</td>\n",
       "      <td>0.060285</td>\n",
       "      <td>0.103735</td>\n",
       "      <td>0.092468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SCHL</th>\n",
       "      <td>0.451920</td>\n",
       "      <td>0.445859</td>\n",
       "      <td>0.329616</td>\n",
       "      <td>0.457014</td>\n",
       "      <td>0.352467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAR</th>\n",
       "      <td>-0.324116</td>\n",
       "      <td>-0.311037</td>\n",
       "      <td>-0.158618</td>\n",
       "      <td>-0.266111</td>\n",
       "      <td>-0.275053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OCCP</th>\n",
       "      <td>-0.473127</td>\n",
       "      <td>-0.470121</td>\n",
       "      <td>-0.757288</td>\n",
       "      <td>-0.514902</td>\n",
       "      <td>-0.341937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>POBP</th>\n",
       "      <td>-0.148944</td>\n",
       "      <td>-0.156099</td>\n",
       "      <td>-0.088740</td>\n",
       "      <td>-0.130747</td>\n",
       "      <td>-0.115937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RELP</th>\n",
       "      <td>-0.281573</td>\n",
       "      <td>-0.249032</td>\n",
       "      <td>-0.107230</td>\n",
       "      <td>-0.250024</td>\n",
       "      <td>-0.207905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WKHP</th>\n",
       "      <td>0.450587</td>\n",
       "      <td>0.442483</td>\n",
       "      <td>0.167823</td>\n",
       "      <td>0.462483</td>\n",
       "      <td>0.333439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SEX</th>\n",
       "      <td>-0.122195</td>\n",
       "      <td>-0.131393</td>\n",
       "      <td>-0.013668</td>\n",
       "      <td>-0.073401</td>\n",
       "      <td>-0.133252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAC1P</th>\n",
       "      <td>-0.139578</td>\n",
       "      <td>-0.150601</td>\n",
       "      <td>-0.076532</td>\n",
       "      <td>-0.118626</td>\n",
       "      <td>-0.151304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pred_gb</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.902014</td>\n",
       "      <td>0.515933</td>\n",
       "      <td>0.876647</td>\n",
       "      <td>0.588425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pred_rf</th>\n",
       "      <td>0.902014</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.514073</td>\n",
       "      <td>0.817851</td>\n",
       "      <td>0.595014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pred_svm</th>\n",
       "      <td>0.515933</td>\n",
       "      <td>0.514073</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.545658</td>\n",
       "      <td>0.350952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pred_ab</th>\n",
       "      <td>0.876647</td>\n",
       "      <td>0.817851</td>\n",
       "      <td>0.545658</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.539679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target</th>\n",
       "      <td>0.588425</td>\n",
       "      <td>0.595014</td>\n",
       "      <td>0.350952</td>\n",
       "      <td>0.539679</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           pred_gb   pred_rf  pred_svm   pred_ab    target\n",
       "AGEP      0.248065  0.245363  0.089803  0.181025  0.296774\n",
       "COW       0.098516  0.073678  0.060285  0.103735  0.092468\n",
       "SCHL      0.451920  0.445859  0.329616  0.457014  0.352467\n",
       "MAR      -0.324116 -0.311037 -0.158618 -0.266111 -0.275053\n",
       "OCCP     -0.473127 -0.470121 -0.757288 -0.514902 -0.341937\n",
       "POBP     -0.148944 -0.156099 -0.088740 -0.130747 -0.115937\n",
       "RELP     -0.281573 -0.249032 -0.107230 -0.250024 -0.207905\n",
       "WKHP      0.450587  0.442483  0.167823  0.462483  0.333439\n",
       "SEX      -0.122195 -0.131393 -0.013668 -0.073401 -0.133252\n",
       "RAC1P    -0.139578 -0.150601 -0.076532 -0.118626 -0.151304\n",
       "pred_gb   1.000000  0.902014  0.515933  0.876647  0.588425\n",
       "pred_rf   0.902014  1.000000  0.514073  0.817851  0.595014\n",
       "pred_svm  0.515933  0.514073  1.000000  0.545658  0.350952\n",
       "pred_ab   0.876647  0.817851  0.545658  1.000000  0.539679\n",
       "target    0.588425  0.595014  0.350952  0.539679  1.000000"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_corr = X_test.copy()\n",
    "\n",
    "y_pred_GB = best_GB.predict(X_test)\n",
    "y_pred_RF = best_RF.predict(X_test)\n",
    "y_pred_SVM = best_SVM.predict(X_test)\n",
    "y_pred_AB = best_AB.predict(X_test)\n",
    "\n",
    "X_test_corr['pred_gb'] = y_pred_GB\n",
    "X_test_corr['pred_rf'] = y_pred_RF\n",
    "X_test_corr['pred_svm'] = y_pred_SVM\n",
    "X_test_corr['pred_ab'] = y_pred_AB\n",
    "X_test_corr['target'] = y_test\n",
    "\n",
    "X_test_corr.corr()[['pred_gb', 'pred_rf', 'pred_svm', 'pred_ab', 'target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7bc0043b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "FIRF = permutation_importance(best_RF, X_train, y_train, n_repeats=10)\n",
    "FIGB = permutation_importance(best_GB, X_train, y_train, n_repeats=10)\n",
    "FISVM = permutation_importance(best_SVM, X_train, y_train, n_repeats=10)\n",
    "FIAB = permutation_importance(best_AB, X_train, y_train, n_repeats=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "550b8d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest\n",
      " [0.06610224 0.01400958 0.07880192 0.01749201 0.08041534 0.02923323\n",
      " 0.04846645 0.1207508  0.01875399 0.01209265] \n",
      "\n",
      "\n",
      "GradientBoosting\n",
      " [0.02587859 0.00188498 0.04892971 0.00268371 0.03766773 0.00726837\n",
      " 0.0207508  0.0907508  0.00669329 0.00111821] \n",
      "\n",
      "\n",
      "SVM\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  1.80287540e-01 -1.59744409e-05  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00] \n",
      "\n",
      "\n",
      "AdaBoost\n",
      " [0.01819489 0.         0.0471246  0.         0.03236422 0.00138978\n",
      " 0.0165655  0.07785942 0.00153355 0.        ] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"RandomForest\\n\",FIRF.importances_mean, \"\\n\\n\")\n",
    "print(\"GradientBoosting\\n\",FIGB.importances_mean, \"\\n\\n\")\n",
    "print(\"SVM\\n\",FISVM.importances_mean, \"\\n\\n\")\n",
    "print(\"AdaBoost\\n\",FIAB.importances_mean, \"\\n\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c375d3e",
   "metadata": {},
   "source": [
    "# SENSITIVE DATA - SEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "de82be0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "datamix = X_test.copy()\n",
    "datamix['label'] = y_test\n",
    "\n",
    "X_test_sex_M = datamix.loc[datamix.SEX == 1.0].drop(columns='label')\n",
    "X_test_sex_F = datamix.loc[datamix.SEX == 2.0].drop(columns='label')\n",
    "\n",
    "y_test_sex_M = datamix.loc[datamix.SEX == 1.0].label\n",
    "y_test_sex_F = datamix.loc[datamix.SEX == 2.0].label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "7941eff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_GB_M = best_GB.predict(X_test_sex_M)\n",
    "y_pred_RF_M = best_RF.predict(X_test_sex_M)\n",
    "y_pred_SVM_M = best_SVM.predict(X_test_sex_M)\n",
    "y_pred_AB_M = best_AB.predict(X_test_sex_M)\n",
    "\n",
    "y_pred_GB_F = best_GB.predict(X_test_sex_F)\n",
    "y_pred_RF_F = best_RF.predict(X_test_sex_F)\n",
    "y_pred_SVM_F = best_SVM.predict(X_test_sex_F)\n",
    "y_pred_AB_F = best_AB.predict(X_test_sex_F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "0e95ae74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GB (M - F)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.79      0.83      0.81       413\n",
      "        True       0.79      0.76      0.78       367\n",
      "\n",
      "    accuracy                           0.79       780\n",
      "   macro avg       0.79      0.79      0.79       780\n",
      "weighted avg       0.79      0.79      0.79       780\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.85      0.86      0.86       519\n",
      "        True       0.73      0.71      0.72       267\n",
      "\n",
      "    accuracy                           0.81       786\n",
      "   macro avg       0.79      0.79      0.79       786\n",
      "weighted avg       0.81      0.81      0.81       786\n",
      "\n",
      "\n",
      "AB (M - F)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.75      0.85      0.80       413\n",
      "        True       0.81      0.68      0.73       367\n",
      "\n",
      "    accuracy                           0.77       780\n",
      "   macro avg       0.78      0.77      0.77       780\n",
      "weighted avg       0.77      0.77      0.77       780\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.83      0.85      0.84       519\n",
      "        True       0.70      0.67      0.69       267\n",
      "\n",
      "    accuracy                           0.79       786\n",
      "   macro avg       0.77      0.76      0.76       786\n",
      "weighted avg       0.79      0.79      0.79       786\n",
      "\n",
      "\n",
      "SVM (M - F)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.66      0.86      0.75       413\n",
      "        True       0.76      0.49      0.60       367\n",
      "\n",
      "    accuracy                           0.69       780\n",
      "   macro avg       0.71      0.68      0.67       780\n",
      "weighted avg       0.71      0.69      0.68       780\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.76      0.81      0.78       519\n",
      "        True       0.57      0.49      0.53       267\n",
      "\n",
      "    accuracy                           0.70       786\n",
      "   macro avg       0.66      0.65      0.66       786\n",
      "weighted avg       0.69      0.70      0.70       786\n",
      "\n",
      "\n",
      "RF (M - F)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.80      0.83      0.82       413\n",
      "        True       0.80      0.77      0.78       367\n",
      "\n",
      "    accuracy                           0.80       780\n",
      "   macro avg       0.80      0.80      0.80       780\n",
      "weighted avg       0.80      0.80      0.80       780\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.85      0.84      0.84       519\n",
      "        True       0.70      0.71      0.70       267\n",
      "\n",
      "    accuracy                           0.80       786\n",
      "   macro avg       0.77      0.78      0.77       786\n",
      "weighted avg       0.80      0.80      0.80       786\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nGB (M - F)\")\n",
    "print(classification_report(y_test_sex_M, y_pred_GB_M))\n",
    "print(classification_report(y_test_sex_F, y_pred_GB_F))\n",
    "\n",
    "print(\"\\nAB (M - F)\")\n",
    "print(classification_report(y_test_sex_M, y_pred_AB_M))\n",
    "print(classification_report(y_test_sex_F, y_pred_AB_F))\n",
    "\n",
    "print(\"\\nSVM (M - F)\")\n",
    "print(classification_report(y_test_sex_M, y_pred_SVM_M))\n",
    "print(classification_report(y_test_sex_F, y_pred_SVM_F))\n",
    "\n",
    "print(\"\\nRF (M - F)\")\n",
    "print(classification_report(y_test_sex_M, y_pred_RF_M))\n",
    "print(classification_report(y_test_sex_F, y_pred_RF_F))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "42de4fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    " # acsincome_ca_features_without_sex\n",
    "# Go search the rest of the data\n",
    "feat_without_sex = pd.read_csv('acsincome_ca_features_without_sex.csv')\n",
    "X_WS, y_WS = preprocess(feat_without_sex, label_raw)\n",
    "\n",
    "X_WS_train, X_WS_test, y_WS_train, y_WS_test = train_test_split(X_WS, y_WS, test_size=0.2)\n",
    "y_WS_train = y_WS_train.to_numpy().reshape(1,-1)[0]\n",
    "\n",
    "for model in best_models:\n",
    "    model.fit(X_WS_train, y_WS_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "d027cf3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================\n",
      "GradientBoostingClassifier(learning_rate=0.03880510732210184,\n",
      "                           loss='exponential', n_estimators=130)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.83      0.82      0.82       938\n",
      "        True       0.73      0.74      0.74       628\n",
      "\n",
      "    accuracy                           0.79      1566\n",
      "   macro avg       0.78      0.78      0.78      1566\n",
      "weighted avg       0.79      0.79      0.79      1566\n",
      "\n",
      "-----------------------------------\n",
      "[[766 172]\n",
      " [161 467]]\n",
      "===================================\n",
      "AdaBoostClassifier(learning_rate=0.05011872336272722, n_estimators=140)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.80      0.84      0.82       938\n",
      "        True       0.75      0.69      0.72       628\n",
      "\n",
      "    accuracy                           0.78      1566\n",
      "   macro avg       0.78      0.77      0.77      1566\n",
      "weighted avg       0.78      0.78      0.78      1566\n",
      "\n",
      "-----------------------------------\n",
      "[[792 146]\n",
      " [192 436]]\n",
      "===================================\n",
      "RandomForestClassifier(criterion='entropy', max_depth=10, n_estimators=130,\n",
      "                       n_jobs=-1)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.83      0.82      0.83       938\n",
      "        True       0.74      0.76      0.75       628\n",
      "\n",
      "    accuracy                           0.79      1566\n",
      "   macro avg       0.79      0.79      0.79      1566\n",
      "weighted avg       0.80      0.79      0.79      1566\n",
      "\n",
      "-----------------------------------\n",
      "[[769 169]\n",
      " [153 475]]\n",
      "===================================\n",
      "SVC(C=0.0107977516232771, degree=2)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.72      0.81      0.77       938\n",
      "        True       0.66      0.53      0.59       628\n",
      "\n",
      "    accuracy                           0.70      1566\n",
      "   macro avg       0.69      0.67      0.68      1566\n",
      "weighted avg       0.70      0.70      0.69      1566\n",
      "\n",
      "-----------------------------------\n",
      "[[763 175]\n",
      " [293 335]]\n"
     ]
    }
   ],
   "source": [
    "for model in best_models:\n",
    "    print(\"===================================\")\n",
    "    print(model)\n",
    "    y_pred = model.predict(X_WS_test)\n",
    "    print(classification_report(y_WS_test, y_pred))\n",
    "    print(\"-----------------------------------\")\n",
    "    print(confusion_matrix(y_WS_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3892c37b",
   "metadata": {},
   "source": [
    "Valeurs d’équité statistique - pas beaucoup de variation, 1% max ! c'est ok."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2896153d",
   "metadata": {},
   "source": [
    "# SENSITIVE DATA - RAC1P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75ee7e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = preprocess(feat_raw, label_raw)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "y_train = y_train.to_numpy().reshape(1,-1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0247a6ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3.934e+03, 2.540e+02, 4.300e+01, 1.000e+00, 0.000e+00, 1.500e+01,\n",
       "        1.015e+03, 1.700e+01, 7.390e+02, 2.420e+02]),\n",
       " array([1. , 1.8, 2.6, 3.4, 4.2, 5. , 5.8, 6.6, 7.4, 8.2, 9. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(X_train['RAC1P'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be0ecf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "datamix = X_test.copy()\n",
    "datamix['label'] = y_test\n",
    "\n",
    "X_test_race_cau = datamix.loc[datamix.RAC1P == 1.0].drop(columns='label')\n",
    "X_test_race_other = datamix.loc[datamix.RAC1P != 1.0].drop(columns='label')\n",
    "\n",
    "y_test_race_cau = datamix.loc[datamix.RAC1P == 1.0].label\n",
    "y_test_race_other = datamix.loc[datamix.RAC1P != 1.0].label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82b210d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_GB = GradientBoostingClassifier(criterion='friedman_mse', learning_rate=0.03880510732210184,\n",
    "                                     loss='exponential', n_estimators=130)\n",
    "\n",
    "best_AB = AdaBoostClassifier(algorithm='SAMME.R', learning_rate=0.05011872336272722, n_estimators=140)\n",
    "\n",
    "best_RF = RandomForestClassifier(criterion='entropy', max_depth=10, min_samples_split=2, n_estimators=130, n_jobs=-1)\n",
    "\n",
    "best_SVM = SVC(C=0.0107977516232771, degree=2, kernel='rbf')\n",
    "        \n",
    "best_models = [best_GB, best_AB, best_RF, best_SVM]\n",
    "\n",
    "for model in best_models:\n",
    "    model.fit(X_train, y_train)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "874c1d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_GB_cau = best_GB.predict(X_test_race_cau)\n",
    "y_pred_RF_cau = best_RF.predict(X_test_race_cau)\n",
    "y_pred_SVM_cau = best_SVM.predict(X_test_race_cau)\n",
    "y_pred_AB_cau = best_AB.predict(X_test_race_cau)\n",
    "\n",
    "y_pred_GB_other = best_GB.predict(X_test_race_other)\n",
    "y_pred_RF_other = best_RF.predict(X_test_race_other)\n",
    "y_pred_SVM_other = best_SVM.predict(X_test_race_other)\n",
    "y_pred_AB_other = best_AB.predict(X_test_race_other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d8f61d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GB (caucasian - other)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.84      0.84      0.84       550\n",
      "        True       0.79      0.79      0.79       415\n",
      "\n",
      "    accuracy                           0.82       965\n",
      "   macro avg       0.81      0.81      0.81       965\n",
      "weighted avg       0.82      0.82      0.82       965\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.84      0.86      0.85       377\n",
      "        True       0.76      0.71      0.74       224\n",
      "\n",
      "    accuracy                           0.81       601\n",
      "   macro avg       0.80      0.79      0.79       601\n",
      "weighted avg       0.81      0.81      0.81       601\n",
      "\n",
      "\n",
      "AB (caucasian - other)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.80      0.86      0.83       550\n",
      "        True       0.79      0.71      0.75       415\n",
      "\n",
      "    accuracy                           0.79       965\n",
      "   macro avg       0.79      0.78      0.79       965\n",
      "weighted avg       0.79      0.79      0.79       965\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.83      0.86      0.84       377\n",
      "        True       0.75      0.69      0.72       224\n",
      "\n",
      "    accuracy                           0.80       601\n",
      "   macro avg       0.79      0.78      0.78       601\n",
      "weighted avg       0.80      0.80      0.80       601\n",
      "\n",
      "\n",
      "SVM (caucasian - other)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.67      0.85      0.75       550\n",
      "        True       0.69      0.43      0.53       415\n",
      "\n",
      "    accuracy                           0.67       965\n",
      "   macro avg       0.68      0.64      0.64       965\n",
      "weighted avg       0.68      0.67      0.65       965\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.74      0.86      0.80       377\n",
      "        True       0.68      0.49      0.57       224\n",
      "\n",
      "    accuracy                           0.72       601\n",
      "   macro avg       0.71      0.68      0.68       601\n",
      "weighted avg       0.72      0.72      0.71       601\n",
      "\n",
      "\n",
      "RF (caucasian - other)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.84      0.83      0.83       550\n",
      "        True       0.78      0.79      0.78       415\n",
      "\n",
      "    accuracy                           0.81       965\n",
      "   macro avg       0.81      0.81      0.81       965\n",
      "weighted avg       0.81      0.81      0.81       965\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.82      0.86      0.84       377\n",
      "        True       0.75      0.69      0.72       224\n",
      "\n",
      "    accuracy                           0.80       601\n",
      "   macro avg       0.79      0.78      0.78       601\n",
      "weighted avg       0.80      0.80      0.80       601\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nGB (caucasian - other)\")\n",
    "print(classification_report(y_test_race_cau, y_pred_GB_cau))\n",
    "print(classification_report(y_test_race_other, y_pred_GB_other))\n",
    "\n",
    "print(\"\\nAB (caucasian - other)\")\n",
    "print(classification_report(y_test_race_cau, y_pred_AB_cau))\n",
    "print(classification_report(y_test_race_other, y_pred_AB_other))\n",
    "\n",
    "print(\"\\nSVM (caucasian - other)\")\n",
    "print(classification_report(y_test_race_cau, y_pred_SVM_cau))\n",
    "print(classification_report(y_test_race_other, y_pred_SVM_other))\n",
    "\n",
    "print(\"\\nRF (caucasian - other)\")\n",
    "print(classification_report(y_test_race_cau, y_pred_RF_cau))\n",
    "print(classification_report(y_test_race_other, y_pred_RF_other))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5a62806f",
   "metadata": {},
   "source": [
    "RF et GB : se trompent + pour les non blancs (8% diff GB et 10% RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "321f33af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go search the rest of the data (without race) and test\n",
    "feat_without_race = pd.read_csv('acsincome_ca_features_without_race.csv')\n",
    "X_WR, y_WR = preprocess(feat_without_race, label_raw)\n",
    "\n",
    "X_WR_train, X_WR_test, y_WR_train, y_WR_test = train_test_split(X_WR, y_WR, test_size=0.2)\n",
    "y_WR_train = y_WR_train.to_numpy().reshape(1,-1)[0]\n",
    "\n",
    "for model in best_models:\n",
    "    model.fit(X_WR_train, y_WR_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "f760015d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================\n",
      "GradientBoostingClassifier(learning_rate=0.03880510732210184,\n",
      "                           loss='exponential', n_estimators=130)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.83      0.81      0.82       938\n",
      "        True       0.73      0.75      0.74       628\n",
      "\n",
      "    accuracy                           0.79      1566\n",
      "   macro avg       0.78      0.78      0.78      1566\n",
      "weighted avg       0.79      0.79      0.79      1566\n",
      "\n",
      "-----------------------------------\n",
      "[[764 174]\n",
      " [159 469]]\n",
      "===================================\n",
      "AdaBoostClassifier(learning_rate=0.05011872336272722, n_estimators=140)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.81      0.84      0.82       938\n",
      "        True       0.75      0.70      0.72       628\n",
      "\n",
      "    accuracy                           0.78      1566\n",
      "   macro avg       0.78      0.77      0.77      1566\n",
      "weighted avg       0.78      0.78      0.78      1566\n",
      "\n",
      "-----------------------------------\n",
      "[[791 147]\n",
      " [190 438]]\n",
      "===================================\n",
      "RandomForestClassifier(criterion='entropy', max_depth=10, n_estimators=130,\n",
      "                       n_jobs=-1)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.84      0.83      0.83       938\n",
      "        True       0.75      0.76      0.76       628\n",
      "\n",
      "    accuracy                           0.80      1566\n",
      "   macro avg       0.80      0.80      0.80      1566\n",
      "weighted avg       0.80      0.80      0.80      1566\n",
      "\n",
      "-----------------------------------\n",
      "[[778 160]\n",
      " [148 480]]\n",
      "===================================\n",
      "SVC(C=0.0107977516232771, degree=2)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.73      0.82      0.77       938\n",
      "        True       0.67      0.55      0.60       628\n",
      "\n",
      "    accuracy                           0.71      1566\n",
      "   macro avg       0.70      0.68      0.69      1566\n",
      "weighted avg       0.71      0.71      0.70      1566\n",
      "\n",
      "-----------------------------------\n",
      "[[768 170]\n",
      " [284 344]]\n"
     ]
    }
   ],
   "source": [
    "for model in best_models:\n",
    "    print(\"===================================\")\n",
    "    print(model)\n",
    "    y_pred = model.predict(X_WR_test)\n",
    "    print(classification_report(y_WR_test, y_pred))\n",
    "    print(\"-----------------------------------\")\n",
    "    print(confusion_matrix(y_WR_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
